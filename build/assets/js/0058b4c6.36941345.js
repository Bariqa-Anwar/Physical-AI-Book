"use strict";(self.webpackChunkdocusourus_book=self.webpackChunkdocusourus_book||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Introduction/Setup","items":[{"type":"link","href":"/docs/introduction/ros2-install","label":"ROS 2 Installation Guide","docId":"introduction/ros2-install","unlisted":false},{"type":"link","href":"/docs/introduction/python-env","label":"Python Environment Setup","docId":"introduction/python-env","unlisted":false},{"type":"link","href":"/docs/introduction/docusaurus-setup","label":"Docusaurus Project Setup","docId":"introduction/docusaurus-setup","unlisted":false},{"type":"link","href":"/docs/introduction/urdf-concepts","label":"Introduction to URDF Concepts","docId":"introduction/urdf-concepts","unlisted":false},{"type":"link","href":"/docs/introduction/rviz2-urdf","label":"Visualizing URDF in RViz2","docId":"introduction/rviz2-urdf","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 1: Foundation","items":[{"type":"link","href":"/docs/module1/ros2-nodes","label":"ROS 2 Nodes and Topics","docId":"module1/ros2-nodes","unlisted":false},{"type":"link","href":"/docs/module1/gazebo-intro","label":"Introduction to Gazebo Simulation","docId":"module1/gazebo-intro","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Sensors & Perception","items":[{"type":"link","href":"/docs/module2/isaac-sim-intro","label":"Introduction to NVIDIA Isaac Sim","docId":"module2/isaac-sim-intro","unlisted":false},{"type":"link","href":"/docs/module2/virtual-sensors","label":"Virtual Sensors in Isaac Sim","docId":"module2/virtual-sensors","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: Control & Action","items":[{"type":"link","href":"/docs/module3/control-fundamentals","label":"Robotic Control Fundamentals","docId":"module3/control-fundamentals","unlisted":false},{"type":"link","href":"/docs/module3/inverse-kinematics","label":"Inverse Kinematics for Robotic Arms","docId":"module3/inverse-kinematics","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: HRI & VLA","items":[{"type":"link","href":"/docs/module4/whisper-stt","label":"Speech-to-Text with OpenAI Whisper","docId":"module4/whisper-stt","unlisted":false},{"type":"link","href":"/docs/module4/gpt-hri","label":"LLM for Command Understanding and Response","docId":"module4/gpt-hri","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"introduction/docusaurus-setup":{"id":"introduction/docusaurus-setup","title":"Docusaurus Project Setup","description":"Docusaurus is a static site generator that helps you build optimized websites quickly. It\'s particularly well-suited for documentation, and we will use it to structure our \\"Physical AI & Humanoid Robotics\\" book. This section covers installing the Docusaurus CLI and creating your project.","sidebar":"tutorialSidebar"},"introduction/python-env":{"id":"introduction/python-env","title":"Python Environment Setup","description":"This section guides you through setting up a dedicated Python environment (using venv or conda) for your ROS 2 projects. Maintaining isolated environments is crucial for managing dependencies and avoiding conflicts between different projects. ROS 2 is built to work with Python 3, and for this book, we will primarily use Python 3.10+.","sidebar":"tutorialSidebar"},"introduction/ros2-install":{"id":"introduction/ros2-install","title":"ROS 2 Installation Guide","description":"This guide provides step-by-step instructions for installing ROS 2 Humble or Iron on Ubuntu, specifically tailored for a Windows Subsystem for Linux (WSL) environment or a native Ubuntu installation.","sidebar":"tutorialSidebar"},"introduction/rviz2-urdf":{"id":"introduction/rviz2-urdf","title":"Visualizing URDF in RViz2","description":"RViz2 (ROS Visualization) is a powerful 3D visualizer for displaying sensor data, robot models, and planning outputs in ROS 2. It\'s an indispensable tool for debugging and understanding your robot\'s behavior. In this section, we\'ll learn how to load and visualize our URDF models in RViz2.","sidebar":"tutorialSidebar"},"introduction/urdf-concepts":{"id":"introduction/urdf-concepts","title":"Introduction to URDF Concepts","description":"The Unified Robot Description Format (URDF) is an XML format for describing all aspects of a robot. It\'s used extensively in ROS to represent the robot\'s kinematic and dynamic properties, visualize the robot in simulators like Gazebo or RViz, and perform motion planning.","sidebar":"tutorialSidebar"},"module1/gazebo-intro":{"id":"module1/gazebo-intro","title":"Introduction to Gazebo Simulation","description":"Gazebo is a powerful 3D robot simulator that is widely used in the robotics community. It allows you to accurately and efficiently simulate populations of robots in complex indoor and outdoor environments. Gazebo offers the ability to simulate sensors, actuators, and even entire robotic systems with realistic physics.","sidebar":"tutorialSidebar"},"module1/ros2-nodes":{"id":"module1/ros2-nodes","title":"ROS 2 Nodes and Topics","description":"In ROS 2, the fundamental unit of computation is a Node. Nodes are processes that perform specific tasks, such as reading sensor data, controlling a motor, or performing calculations. Nodes communicate with each other by sending and receiving Messages over Topics. This publish/subscribe messaging pattern is central to ROS 2\'s distributed architecture.","sidebar":"tutorialSidebar"},"module2/isaac-sim-intro":{"id":"module2/isaac-sim-intro","title":"Introduction to NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a scalable, physically accurate robotics simulation application and development platform for building, testing, and managing AI-based robots. Built on NVIDIA Omniverse, it provides a powerful environment for high-fidelity simulation, which is crucial for developing and validating complex robotics applications.","sidebar":"tutorialSidebar"},"module2/virtual-sensors":{"id":"module2/virtual-sensors","title":"Virtual Sensors in Isaac Sim","description":"NVIDIA Isaac Sim excels at providing highly realistic and configurable virtual sensors, which are crucial for developing and testing perception algorithms without the need for physical hardware. These virtual sensors can generate data streams that closely mimic real-world sensors, and through the ROS 2 Bridge, this data can be seamlessly published to ROS 2 topics.","sidebar":"tutorialSidebar"},"module3/control-fundamentals":{"id":"module3/control-fundamentals","title":"Robotic Control Fundamentals","description":"Controlling a robot to perform desired actions is at the heart of robotics. This module delves into fundamental control concepts, focusing on how to make robots move precisely and effectively. We\'ll explore commonly used control algorithms and the mathematical tools necessary to understand and implement them, such as PID control and the Jacobian matrix for kinematics.","sidebar":"tutorialSidebar"},"module3/inverse-kinematics":{"id":"module3/inverse-kinematics","title":"Inverse Kinematics for Robotic Arms","description":"Inverse Kinematics (IK) is a fundamental problem in robotics that involves determining the joint configurations of a robot arm that will achieve a desired position and orientation for its end-effector. While forward kinematics calculates the end-effector pose from known joint angles, IK solves the inverse: Given (x, y, z, roll, pitch, yaw) of end-effector, find (q1, q2, ..., qn) joint angles.","sidebar":"tutorialSidebar"},"module4/gpt-hri":{"id":"module4/gpt-hri","title":"LLM for Command Understanding and Response","description":"Once speech has been converted to text using a Speech-to-Text (STT) model like Whisper, the next step in achieving natural Human-Robot Interaction (HRI) is to interpret these text commands and generate an appropriate response or action. Large Language Models (LLMs) like OpenAI\'s GPT series are exceptionally good at understanding natural language, extracting intent, and generating coherent text, making them ideal for this role.","sidebar":"tutorialSidebar"},"module4/whisper-stt":{"id":"module4/whisper-stt","title":"Speech-to-Text with OpenAI Whisper","description":"Integrating natural language interaction into robotic systems is a key step towards more intuitive Human-Robot Interaction (HRI). One fundamental component of this is Speech-to-Text (STT), which converts spoken language into written text that the robot\'s systems can process. OpenAI\'s Whisper model is a powerful general-purpose speech recognition model capable of transcribing audio into text with high accuracy.","sidebar":"tutorialSidebar"}}}}')}}]);